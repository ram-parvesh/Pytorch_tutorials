{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3510b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28d15e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "batch_size=100\n",
    "lr=0.001\n",
    "\n",
    "train_dataset = dsets.MNIST(root=\"./data\",\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "\n",
    "test_dataset = dsets.MNIST(root=\"./data\",\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f9d8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(                    #input size (1,28,28)\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2), #in_channel=1,out_channel=16,filter size=5\n",
    "            nn.BatchNorm2d(16),     # o/p size (16,28,28)\n",
    "            nn.ReLU(),              #activation  o/p size  (16,28,28)\n",
    "            nn.MaxPool2d(2))        #output size after pooling (16,14,14)\n",
    "        \n",
    "        \n",
    "        self.layer2 = nn.Sequential(    #input shape (16,14,14)\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))          #output shape (32,7,7)\n",
    "        \n",
    "        self.fc = nn.Linear(32*7*7,10)     # fully connected Layer,output 10 classes\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        out = out.view(out.size(0),-1)    #flatten the output of conv2d to feed into fully connected layers\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out   \n",
    "\n",
    "\n",
    "cnn = CNN()\n",
    "# print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8df1f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10] ,iter [100 / 600] Loss:0.0587\n",
      "Epoch [1 / 10] ,iter [200 / 600] Loss:0.0188\n",
      "Epoch [1 / 10] ,iter [300 / 600] Loss:0.0397\n",
      "Epoch [1 / 10] ,iter [400 / 600] Loss:0.0265\n",
      "Epoch [1 / 10] ,iter [500 / 600] Loss:0.0430\n",
      "Epoch [1 / 10] ,iter [600 / 600] Loss:0.0233\n",
      "Epoch [2 / 10] ,iter [100 / 600] Loss:0.0527\n",
      "Epoch [2 / 10] ,iter [200 / 600] Loss:0.0100\n",
      "Epoch [2 / 10] ,iter [300 / 600] Loss:0.0228\n",
      "Epoch [2 / 10] ,iter [400 / 600] Loss:0.0028\n",
      "Epoch [2 / 10] ,iter [500 / 600] Loss:0.0032\n",
      "Epoch [2 / 10] ,iter [600 / 600] Loss:0.0146\n",
      "Epoch [3 / 10] ,iter [100 / 600] Loss:0.0288\n",
      "Epoch [3 / 10] ,iter [200 / 600] Loss:0.0117\n",
      "Epoch [3 / 10] ,iter [300 / 600] Loss:0.0017\n",
      "Epoch [3 / 10] ,iter [400 / 600] Loss:0.0366\n",
      "Epoch [3 / 10] ,iter [500 / 600] Loss:0.0188\n",
      "Epoch [3 / 10] ,iter [600 / 600] Loss:0.0180\n",
      "Epoch [4 / 10] ,iter [100 / 600] Loss:0.0070\n",
      "Epoch [4 / 10] ,iter [200 / 600] Loss:0.0336\n",
      "Epoch [4 / 10] ,iter [300 / 600] Loss:0.0226\n",
      "Epoch [4 / 10] ,iter [400 / 600] Loss:0.0070\n",
      "Epoch [4 / 10] ,iter [500 / 600] Loss:0.0040\n",
      "Epoch [4 / 10] ,iter [600 / 600] Loss:0.0059\n",
      "Epoch [5 / 10] ,iter [100 / 600] Loss:0.0019\n",
      "Epoch [5 / 10] ,iter [200 / 600] Loss:0.0073\n",
      "Epoch [5 / 10] ,iter [300 / 600] Loss:0.0008\n",
      "Epoch [5 / 10] ,iter [400 / 600] Loss:0.0088\n",
      "Epoch [5 / 10] ,iter [500 / 600] Loss:0.0111\n",
      "Epoch [5 / 10] ,iter [600 / 600] Loss:0.0430\n",
      "Epoch [6 / 10] ,iter [100 / 600] Loss:0.0139\n",
      "Epoch [6 / 10] ,iter [200 / 600] Loss:0.0005\n",
      "Epoch [6 / 10] ,iter [300 / 600] Loss:0.0021\n",
      "Epoch [6 / 10] ,iter [400 / 600] Loss:0.0126\n",
      "Epoch [6 / 10] ,iter [500 / 600] Loss:0.0070\n",
      "Epoch [6 / 10] ,iter [600 / 600] Loss:0.0148\n",
      "Epoch [7 / 10] ,iter [100 / 600] Loss:0.0046\n",
      "Epoch [7 / 10] ,iter [200 / 600] Loss:0.0032\n",
      "Epoch [7 / 10] ,iter [300 / 600] Loss:0.0029\n",
      "Epoch [7 / 10] ,iter [400 / 600] Loss:0.0462\n",
      "Epoch [7 / 10] ,iter [500 / 600] Loss:0.0046\n",
      "Epoch [7 / 10] ,iter [600 / 600] Loss:0.0104\n",
      "Epoch [8 / 10] ,iter [100 / 600] Loss:0.0004\n",
      "Epoch [8 / 10] ,iter [200 / 600] Loss:0.0750\n",
      "Epoch [8 / 10] ,iter [300 / 600] Loss:0.0095\n",
      "Epoch [8 / 10] ,iter [400 / 600] Loss:0.0047\n",
      "Epoch [8 / 10] ,iter [500 / 600] Loss:0.0010\n",
      "Epoch [8 / 10] ,iter [600 / 600] Loss:0.0198\n",
      "Epoch [9 / 10] ,iter [100 / 600] Loss:0.0031\n",
      "Epoch [9 / 10] ,iter [200 / 600] Loss:0.0053\n",
      "Epoch [9 / 10] ,iter [300 / 600] Loss:0.0033\n",
      "Epoch [9 / 10] ,iter [400 / 600] Loss:0.0017\n",
      "Epoch [9 / 10] ,iter [500 / 600] Loss:0.0003\n",
      "Epoch [9 / 10] ,iter [600 / 600] Loss:0.0040\n",
      "Epoch [10 / 10] ,iter [100 / 600] Loss:0.0013\n",
      "Epoch [10 / 10] ,iter [200 / 600] Loss:0.0011\n",
      "Epoch [10 / 10] ,iter [300 / 600] Loss:0.0360\n",
      "Epoch [10 / 10] ,iter [400 / 600] Loss:0.0017\n",
      "Epoch [10 / 10] ,iter [500 / 600] Loss:0.0074\n",
      "Epoch [10 / 10] ,iter [600 / 600] Loss:0.0070\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"pkl/cnn.pkl\"):\n",
    "    cnn.load_state_dict(torch.load(\"pkl/cnn.pkl\"))\n",
    "\n",
    "\n",
    "else:\n",
    "    criterian = nn.CrossEntropyLoss() #LOSS\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images,labels) in enumerate(train_loader):   # gives batch data ,normalize x when iterate train_loader\n",
    "            \n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            optimizer.zero_grad()    #clear gradients for this training step\n",
    "            \n",
    "            outputs=cnn(images)     #cnn  output\n",
    "            \n",
    "            loss=criterian(outputs,labels)  #cross entropy loss\n",
    "            \n",
    "            loss.backward()      #backpropagation ,compute gradients\n",
    "            \n",
    "            optimizer.step()     #apply gradients\n",
    "            \n",
    "            if (i+1) % 100 ==0:\n",
    "                print(\"Epoch [%d / %d] ,iter [%d / %d] Loss:%.4f\"\n",
    "                      %(epoch+1,num_epochs,i+1,len(train_dataset)//batch_size,loss.data))\n",
    "                if not os.path.isfile(\"pkl/cnn.pkl\"):\n",
    "                    torch.save(cnn.state_dict(),\"pkl/cnn.pkl\")\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cde0e2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 10000 test images = 99.129997 %\n"
     ]
    }
   ],
   "source": [
    "cnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images ,labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    \n",
    "    _,predicted = torch.max(outputs.data,1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted ==labels).sum()\n",
    "    \n",
    "print(\"Test Accuracy 10000 test images = %f %%\" %(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9a41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
